{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88561fd4-aa53-4912-9301-e7f887f28ddd",
   "metadata": {},
   "source": [
    "# Formatting Anki Flashcards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16655e-0278-4a9e-9d3e-414bdeeae376",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this notebook, we are exploring the best way to prompt an LLM to improve the formatting of Anki flashcards. \n",
    "\n",
    "Approaches we are including:\n",
    "1. Simple prompt\n",
    "2. Simple prompt + Chain of Thought\n",
    "3. Two-step process (critique â†’ refine)\n",
    "4. Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b40c65b-b584-47c7-9094-24187a434858",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471e4473-274b-491a-ae26-fa61d07b715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from anki.collection import Collection\n",
    "\n",
    "from addon.application.use_cases.note_counter import is_note_marked_for_review\n",
    "from addon.infrastructure.configuration.settings import AddonConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a631ea3e-b3b2-471d-8395-30c898b07a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes: 3362\n",
      "Number of cards: 3498\n"
     ]
    }
   ],
   "source": [
    "# Open an existing collection\n",
    "col = Collection(\"/home/gianluca/.local/share/Anki2/User 1/collection.anki2\")\n",
    "\n",
    "# Do something with the collection\n",
    "print(f\"Number of notes: {col.note_count()}\")\n",
    "print(f\"Number of cards: {col.card_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859fd3-c78b-44f3-b03c-d4d565446970",
   "metadata": {},
   "source": [
    "### Connect to Inference Server\n",
    "\n",
    "For this notebook, we are going to use [`unsloth/Qwen3-14B-GGUF`](https://huggingface.co/unsloth/Qwen3-14B-GGUF). This is a larger and more modern LLM compared to [`meta-llama/Llama-3.1-8B-Instruct`](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), which should lead to better results. We are also going to use the Chat Completion API, which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed266c25-a5ae-4cff-b374-87fc37cf7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Mode(Enum):\n",
    "    \"\"\"Response API not implemented since currently not supported by vLLM.\"\"\"\n",
    "\n",
    "    COMPLETION = \"v1/completions\"\n",
    "    CHAT_COMPLETION = \"v1/chat/completions\"\n",
    "\n",
    "\n",
    "def answer(\n",
    "    prompt: str,\n",
    "    guided_json = None,\n",
    "    mode: Mode = Mode.CHAT_COMPLETION,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Helper function to prompt LLM.\n",
    "\n",
    "    Handles both Chat Completion and Completion API format.\n",
    "    kwargs can override config values like max_tokens, temperature, etc.\n",
    "    \"\"\"\n",
    "    # Overwrite the default API mode and any other kwargs\n",
    "    config_overrides = {\"mode\": mode.value, **kwargs}\n",
    "    config = AddonConfig.create_nullable(config_overrides)\n",
    "    \n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        prompt_param_name = \"messages\"\n",
    "        prompt_param_value = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    else:\n",
    "        prompt_param_name = \"prompt\"\n",
    "        prompt_param_value = prompt\n",
    "\n",
    "    payload = {\n",
    "        \"model\": config.model_name,\n",
    "        prompt_param_name: prompt_param_value,\n",
    "        \"max_tokens\": config.max_tokens,\n",
    "        \"temperature\": config.temperature,\n",
    "        \"top_p\": config.top_p,\n",
    "        \"min_p\": config.min_p,\n",
    "        \"top_k\": config.top_k,\n",
    "    }\n",
    "    \n",
    "    if guided_json is not None:\n",
    "        payload[\"guided_json\"] = guided_json\n",
    "\n",
    "    response = requests.post(config.url, json=payload)\n",
    "    print(response)\n",
    "    \n",
    "    # Check for errors\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error response: {response.text}\")\n",
    "        raise Exception(f\"Server returned {response.status_code}: {response.text}\")\n",
    "\n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        reasoning_content = response.json()[\"choices\"][0][\"message\"][\n",
    "            \"reasoning_content\"\n",
    "        ]\n",
    "        content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(f\"Content: {content.replace('<think>\\n\\n</think>\\n\\n', '')}\")\n",
    "        print(f\"Reasoning: {reasoning_content}\")\n",
    "        return (content, reasoning_content)\n",
    "\n",
    "    if mode == Mode.COMPLETION:\n",
    "        content = response.json()[\"choices\"][0][\"text\"]\n",
    "        print(f\"Content: {content}\")\n",
    "        return (content, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2acce-c7a3-4a2e-94d9-dc27aac943d5",
   "metadata": {},
   "source": [
    "### Completions vs Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efc5e84-5fb7-4963-9e13-3626e4b3c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: ciao\n",
      "Reasoning: None\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    prompt=\"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.CHAT_COMPLETION, \n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94104257-3345-419d-a15c-c899890dcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "# The user is asking for the Italian word for 'hello'. I need to provide the correct\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION, \n",
    "    max_tokens=20\n",
    ",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d079c9f-e977-4ab4-9086-7fd353e8fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION, \n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db56374-238e-4da5-8239-eb4ddb729c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "</think>\n",
      "\n",
      "ciao\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION, \n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b7e3c-933d-48c3-b2b9-ec80e0237402",
   "metadata": {},
   "source": [
    "For Qwen 3, the Chat Completions API appears to work much better. In our simple test case, when using the Completions API, Qwen 3 tends to enter a pattern where it repeats itself until reaching the `max_tokens` limit.\n",
    "\n",
    "We also notice that, despite asking Qwen 3 not to output \"thinking tokens\", it still does that in the `content` field. The thinking tokens are also returned both in the `content` and `reasoning` fields and do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb2684-439a-4a96-90dd-266dbfee8d38",
   "metadata": {},
   "source": [
    "## Select a few flashcards for our offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3f1b-7b16-4c22-a8ba-c78109a2fa04",
   "metadata": {},
   "source": [
    "We have everything we need now to tell the LLM to make some changes to our Anki flashcards.\n",
    "\n",
    "Let's pull a few note currently marked for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4449c78a-c525-43b3-96f4-cf665617105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_id = col.decks.current()[\"id\"]\n",
    "query = f\"did:{deck_id}\"\n",
    "note_ids = col.find_notes(query)\n",
    "\n",
    "NUM_NOTES_NEEDED = 10\n",
    "\n",
    "flagged_notes = []\n",
    "for note_id in note_ids:\n",
    "    if is_note_marked_for_review(col, note_id):\n",
    "        note = col.get_note(note_id)\n",
    "        flagged_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cad38b4-78b1-4790-8e4b-b2a77e763c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flagged notes: 284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of flagged notes: {len(flagged_notes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d6db93-fc16-43bb-b348-1e49e7df1e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)\n"
     ]
    }
   ],
   "source": [
    "from addon.application.services.formatter_service import AnkiNoteAdapter\n",
    "\n",
    "addon_note = AnkiNoteAdapter.to_addon_note(flagged_notes[0])\n",
    "print(addon_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af649819-1267-4ea3-910a-aac5265734aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\\n\\nAddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\\n\\n{addon_note}\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec669fd-203e-439c-8f94-75b9428c3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: <think>\n",
      "Okay, let's see. The user provided a flashcard about whether bagging can be performed in parallel. The front is the question, back is \"Yes\", and it's tagged as ml. The user wants to know how to improve this flashcard.\n",
      "\n",
      "First, I need to remember what bagging is. Bagging, or bootstrap aggregating, involves training multiple models on different subsets of the data and then combining their predictions. Each model is trained independently, right? So, if they're independent, can they be done in parallel?\n",
      "\n",
      "The current answer is \"Yes\", but maybe that's too brief. The user mentioned flashcards should be atomic, concise, and accurate. So maybe the answer is correct, but perhaps it's missing some context. For example, explaining why it's possible. But the user wants it concise, so maybe not. However, the question is about whether it can be done, not why. So the answer is correct.\n",
      "\n",
      "But maybe the front of the card is a bit vague. The question is \"Can bagging be performed in parallel?\" which is a yes/no question. But maybe it's better to make it more specific. For example, \"Is bagging inherently parallelizable?\" or \"Can the models in bagging be trained in parallel?\" That might make the question clearer.\n",
      "\n",
      "Also, the tags are 'ml', which is correct. The notetype is basic, which is fine. The deck name is None, but that's probably okay.\n",
      "\n",
      "Another point: the answer is \"Yes\", but maybe it's better to add a brief explanation. However, the user said the flashcards should be atomic and concise. So maybe the answer is fine as is. But if the user is using Anki or similar, sometimes having a bit more detail helps, but the user might prefer it to be as minimal as possible.\n",
      "\n",
      "Wait, the user is asking how to improve the flashcard. So possible improvements could be:\n",
      "\n",
      "1. Make the question more precise. Instead of \"Can bagging be performed in parallel?\", maybe \"Are the individual models in bagging trained independently and thus parallelizable?\" That makes it clearer that the parallelism is due to independence.\n",
      "\n",
      "2. The answer could be expanded slightly to \"Yes, because each model in bagging is trained on a different bootstrap sample, allowing parallel processing.\" But the user wants it concise. So maybe that's too much.\n",
      "\n",
      "3. Check if the answer is accurate. Since bagging involves training models on different subsets, which are independent, yes, they can be done in parallel. So the answer is correct.\n",
      "\n",
      "Alternatively, maybe the answer should be \"Yes, since each model is trained independently on different bootstrap samples.\" But again, the user might prefer brevity.\n",
      "\n",
      "Another angle: the term \"bagging\" itself refers to the process of creating bootstrap samples and aggregating. So the parallelism is inherent in the method. So the answer is correct.\n",
      "\n",
      "So possible improvements are:\n",
      "\n",
      "- Refine the question for clarity.\n",
      "- Ensure the answer is accurate and concise.\n",
      "\n",
      "But the original answer is correct. Maybe the user is okay with it as is, but perhaps the question could be phrased better. For example, \"Can the individual models in a bagging ensemble be trained in parallel?\" which is more specific.\n",
      "\n",
      "Alternatively, if the user is using this for a study session, the current version is sufficient, but adding a bit more context might help retention. However, the user's instruction is to make it atomic and concise, so perhaps not.\n",
      "\n",
      "In conclusion, the flashcard is mostly correct, but the question could be slightly rephrased for clarity. The answer is accurate but could be slightly expanded if allowed, but given the constraints, it's probably fine.\n",
      "</think>\n",
      "\n",
      "The flashcard is concise and accurate, but it can be improved for clarity and precision. Here's a refined version:\n",
      "\n",
      "**Front:**  \n",
      "\"Can the individual models in a bagging ensemble be trained in parallel?\"  \n",
      "\n",
      "**Back:**  \n",
      "\"Yes, because each model is trained on independent bootstrap samples.\"  \n",
      "\n",
      "**Improvements:**  \n",
      "1. **Clarity:** The question now specifies \"individual models in a bagging ensemble\" to avoid ambiguity about what \"bagging\" refers to.  \n",
      "2. **Precision:** The answer adds a brief rationale (\"independent bootstrap samples\") to explain why parallelization is possible, enhancing understanding without sacrificing conciseness.  \n",
      "3. **Atomicity:** The revised card remains focused on a single concept (parallel training in bagging) while providing context for accuracy.  \n",
      "\n",
      "This version maintains brevity while improving pedagogical value.\n",
      "Reasoning: None\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    prompt=prompt,\n",
    "    mode=Mode.CHAT_COMPLETION, \n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bc6f5-2c81-4291-af65-371c76c4ee56",
   "metadata": {},
   "source": [
    "The model is doing a good job, at least on this flashcard. However, we probably need some structured output/constrained decoding to ensure the LLM respects a predefined format. This will make extracting the suggestions more easier.\n",
    "\n",
    "Let's reuse the `pydantic` schema we have already defined for the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12f4cd6-cb99-4f58-b27f-5857a00d474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: {\n",
      "\n",
      "\n",
      "\"front\": \"Can bagging be performed in parallel?\",\n",
      "\"back\": \"Yes, bagging can be performed in parallel because each model in the ensemble is trained independently on different subsets of the data.\",\n",
      "\"tags\": [\"ml\", \"bagging\", \"parallelism\"]\n",
      "}\n",
      "Reasoning: None\n"
     ]
    }
   ],
   "source": [
    "from addon.infrastructure.llm.schemas import AddonNoteChanges\n",
    "\n",
    "content, reasoning_content = answer(\n",
    "    prompt=prompt,\n",
    "    guided_json=AddonNoteChanges.model_json_schema(),\n",
    "    mode=Mode.CHAT_COMPLETION, \n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60e007-968e-4e19-a75d-2ac241a44253",
   "metadata": {},
   "source": [
    "Not bad! Let's compare it with the current version we have in the `main` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1a3de30-f1f3-4a53-8859-ccfa8bb60d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from addon.infrastructure.external_services.openai import OpenAIClient\n",
    "from addon.application.services.formatter_service import NoteFormatter\n",
    "\n",
    "config = AddonConfig.create_nullable()\n",
    "openai = OpenAIClient.create(config)\n",
    "formatter = NoteFormatter(openai)\n",
    "\n",
    "formatter.format(note=addon_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa057295-7dc6-492b-9f96-8181f468fbda",
   "metadata": {},
   "source": [
    "The current version in productin basically does not make any change to the original flashcard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6541e02d-ceb4-4629-954b-6d65f9ea8fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addon_note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05200894-9109-42c0-b4f8-825cda86c8e7",
   "metadata": {},
   "source": [
    "Let's try to let the LLM reason, while still perform constrained decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c3efe11-b155-4e4c-b7b3-5c05ef2725b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: {\"front\": \"Can Bootstrap Aggregating (bagging) be performed in parallel?\", \"back\": \"Yes, because each model in bagging is trained independently on different bootstrap samples, allowing parallel processing.\", \"tags\": [\"ml\", \"ensemble_methods\", \"bagging\"]}\n",
      "Reasoning: \n",
      "Okay, let's see. The user is asking how to improve a flashcard. The original flashcard is about whether bagging can be performed in parallel. The answer is \"Yes,\" and the tags are 'ml'. \n",
      "\n",
      "First, I need to check if the question is atomic and concise. The question is \"Can bagging be performed in parallel?\" That's pretty straightforward. But maybe it can be more specific. Bagging refers to Bootstrap Aggregating, right? So maybe the question should mention that to avoid ambiguity.\n",
      "\n",
      "The answer is just \"Yes.\" That's too short. Flashcards should have more detailed answers. Why is the answer \"Yes\"? Because each model in bagging is trained independently on different subsets of data, which allows parallel processing. So the back of the card should explain that reasoning.\n",
      "\n",
      "Tags are 'ml', which is good, but maybe adding 'ensemble_methods' or 'bagging' could be more specific. Also, the notetype is basic, which is fine, but maybe a more structured note type would help, like a definition or explanation type.\n",
      "\n",
      "The deck name is None, which might not be an issue if it's part of a larger deck, but it's better to have a clear deck name for organization.\n",
      "\n",
      "So, the improvements would be: make the question more precise by including \"Bootstrap Aggregating (bagging)\" to clarify the term. Expand the answer to explain why it's possible, mentioning independent training and parallel processing. Add more specific tags like 'ensemble_methods' and 'bagging'. Ensure the notetype is appropriate, and maybe specify a deck name if needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from addon.infrastructure.llm.schemas import AddonNoteChanges\n",
    "\n",
    "prompt = f\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate. \\think \\n\\n{addon_note}\"\n",
    "content, reasoning_content = answer(\n",
    "    prompt=prompt,\n",
    "    guided_json=AddonNoteChanges.model_json_schema(),\n",
    "    mode=Mode.CHAT_COMPLETION, \n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c735cfe6-db80-45d6-89d3-72acf9cbaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01900f2a-bea3-45ec-b277-8c8166e607f1",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Check Qwen 3's instruction following capabilities with Completions and Chat Completions API\n",
    "- [ ] Simple prompt\n",
    "- [ ] Simple prompt + Chain of Thought\n",
    "- [ ] Two-step process (critique â†’ refine)\n",
    "- [ ] Agent\n",
    "- [ ] Check if we have a class that we can use to read the collection from the hard disk and convert it to `AddonNote` instead of `Note`. In that way we can operate with domain objects instead of external dependencies. The same class should be used to convert the `AddonNote` back to `Note` (so maybe we should keep track of the `note_id`\n",
    "- [ ] Once we have that class, we should update the rest of the codebase accordingly (e.g., note formatter, note counter, etc.)\n",
    "- [ ] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c8e77-6198-4a26-b303-65546c7249cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
