{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3962a900-4a4c-4360-8f41-a9b0de4d6d18",
   "metadata": {},
   "source": [
    "# Formatting Anki Flashcards\n",
    "\n",
    "In this notebook, we are exploring the best way to prompt an LLM to improve the formatting of Anki flashcards. \n",
    "\n",
    "Approaches we are including:\n",
    "1. Simple prompt\n",
    "2. Simple prompt + Chain of Thought\n",
    "3. Two-step process (critique â†’ refine)\n",
    "4. Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b40c65b-b584-47c7-9094-24187a434858",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471e4473-274b-491a-ae26-fa61d07b715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from anki.collection import Collection\n",
    "\n",
    "from addon.application.use_cases.note_counter import is_note_marked_for_review\n",
    "from addon.infrastructure.configuration.settings import AddonConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a631ea3e-b3b2-471d-8395-30c898b07a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes: 3362\n",
      "Number of cards: 3498\n"
     ]
    }
   ],
   "source": [
    "# Open an existing collection\n",
    "col = Collection(\"/home/gianluca/.local/share/Anki2/User 1/collection.anki2\")\n",
    "\n",
    "# Do something with the collection\n",
    "print(f\"Number of notes: {col.note_count()}\")\n",
    "print(f\"Number of cards: {col.card_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859fd3-c78b-44f3-b03c-d4d565446970",
   "metadata": {},
   "source": [
    "### Connect to Inference Server\n",
    "\n",
    "For this notebook, we are going to use [`unsloth/Qwen3-14B-GGUF`](https://huggingface.co/unsloth/Qwen3-14B-GGUF). This is a larger and more modern LLM compared to [`meta-llama/Llama-3.1-8B-Instruct`](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), which should lead to better results. We are also going to use the Chat Completion API, which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed266c25-a5ae-4cff-b374-87fc37cf7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Mode(Enum):\n",
    "    \"\"\"Response API not implemented since currently not supported by vLLM.\"\"\"\n",
    "\n",
    "    COMPLETION = \"v1/completions\"\n",
    "    CHAT_COMPLETION = \"v1/chat/completions\"\n",
    "\n",
    "\n",
    "def answer(\n",
    "    prompt,\n",
    "    mode: Mode = Mode.CHAT_COMPLETION,\n",
    "):\n",
    "    \"\"\"Helper function to prompt LLM.\n",
    "\n",
    "    Handles both Chat Completion and Completion API format.\n",
    "    \"\"\"\n",
    "    # Overwrite the default API mode\n",
    "    config = AddonConfig.create_nullable({\"mode\": mode.value})\n",
    "\n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        prompt_param_name = \"messages\"\n",
    "        prompt_param_value = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    else:\n",
    "        prompt_param_name = \"prompt\"\n",
    "        prompt_param_value = prompt\n",
    "\n",
    "    payload = {\n",
    "        \"model\": config.model_name,\n",
    "        prompt_param_name: prompt_param_value,\n",
    "        \"max_tokens\": config.max_tokens,\n",
    "        \"temperature\": config.temperature,\n",
    "        \"top_p\": config.top_p,\n",
    "        \"min_p\": config.min_p,\n",
    "        \"top_k\": config.top_k,\n",
    "    }\n",
    "\n",
    "    response = requests.post(config.url, json=payload)\n",
    "\n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        print(response)\n",
    "        reasoning_content = response.json()[\"choices\"][0][\"message\"][\n",
    "            \"reasoning_content\"\n",
    "        ]\n",
    "        content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(f\"Content: {content.replace('<think>\\n\\n</think>\\n\\n', '')}\")\n",
    "        print(f\"Reasoning: {reasoning_content}\")\n",
    "        return (content, reasoning_content)\n",
    "    elif mode == Mode.COMPLETION:\n",
    "        print(response)\n",
    "        content = response.json()[\"choices\"][0][\"text\"]\n",
    "        print(f\"Content: {content}\")\n",
    "        return (content, None)\n",
    "    else:\n",
    "        raise ValueError(f\"{mode.value} not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b7e3c-933d-48c3-b2b9-ec80e0237402",
   "metadata": {},
   "source": [
    "It seems that the Chat Completion API works much better for Qwen3. At least, on this simple test case. I still do not understand why the thinking tokens are returned both in the `content` and `reasoning` fields. Even more interestingly, they do not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efc5e84-5fb7-4963-9e13-3626e4b3c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: ciao\n",
      "Reasoning: None\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94104257-3345-419d-a15c-c899890dcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "</think>\n",
      "\n",
      "ciao\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d079c9f-e977-4ab4-9086-7fd353e8fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Okay, the user is asking for the Italian word for 'hello'. Let me think. I know that in Italian, \"ciao\" is commonly used as a greeting. It's like how \"hi\" or \"hello\" is used in English. But wait, are there other words too? Like \"buongiorno\" for good day, but that's more formal. \"Salve\" is another one, but maybe less common. However, \"ciao\" is the most straightforward and widely recognized term. The user specified to respond with one word, lowercase, no punctuation. So \"ciao\" fits perfectly. I should make sure there's no typo. Yep, that's correct. No need to overcomplicate. Just \"ciao\".\n",
      "</think>\n",
      "\n",
      "ciao\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db56374-238e-4da5-8239-eb4ddb729c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3f1b-7b16-4c22-a8ba-c78109a2fa04",
   "metadata": {},
   "source": [
    "We have everything we need now to tell the LLM to make some changes to our Anki flashcards.\n",
    "\n",
    "Let's pull a few note currently marked for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4449c78a-c525-43b3-96f4-cf665617105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_id = col.decks.current()[\"id\"]\n",
    "query = f\"did:{deck_id}\"\n",
    "note_ids = col.find_notes(query)\n",
    "\n",
    "NUM_NOTES_NEEDED = 10\n",
    "\n",
    "flagged_notes = []\n",
    "for note_id in note_ids:\n",
    "    if is_note_marked_for_review(col, note_id):\n",
    "        note = col.get_note(note_id)\n",
    "        flagged_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cad38b4-78b1-4790-8e4b-b2a77e763c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flagged notes: 284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of flagged notes: {len(flagged_notes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c735cfe6-db80-45d6-89d3-72acf9cbaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01900f2a-bea3-45ec-b277-8c8166e607f1",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "- [ ] Check if we have a class that we can use to read the collection from the hard disk and convert it to `AddonNote` instead of `Note`. In that way we can operate with domain objects instead of external dependencies. The same class should be used to convert the `AddonNote` back to `Note` (so maybe we should keep track of the `note_id`\n",
    "- [ ] Once we have that class, we should update the rest of the codebase accordingly (e.g., note formatter, note counter, etc.)\n",
    "- [ ] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c8e77-6198-4a26-b303-65546c7249cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
