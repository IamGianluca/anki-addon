{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88561fd4-aa53-4912-9301-e7f887f28ddd",
   "metadata": {},
   "source": [
    "# Formatting Anki Flashcards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16655e-0278-4a9e-9d3e-414bdeeae376",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this notebook, we are exploring the best way to prompt an LLM to improve the formatting of Anki flashcards. \n",
    "\n",
    "Approaches we are including:\n",
    "1. Simple prompt\n",
    "2. Simple prompt + Chain of Thought\n",
    "3. Two-step process (critique → refine)\n",
    "4. Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b40c65b-b584-47c7-9094-24187a434858",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471e4473-274b-491a-ae26-fa61d07b715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from anki.collection import Collection\n",
    "\n",
    "from addon.application.use_cases.note_counter import is_note_marked_for_review\n",
    "from addon.infrastructure.configuration.settings import AddonConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a631ea3e-b3b2-471d-8395-30c898b07a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes: 3362\n",
      "Number of cards: 3498\n"
     ]
    }
   ],
   "source": [
    "# Open an existing collection\n",
    "col = Collection(\"/home/gianluca/.local/share/Anki2/User 1/collection.anki2\")\n",
    "\n",
    "# Do something with the collection\n",
    "print(f\"Number of notes: {col.note_count()}\")\n",
    "print(f\"Number of cards: {col.card_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859fd3-c78b-44f3-b03c-d4d565446970",
   "metadata": {},
   "source": [
    "### Connect to Inference Server\n",
    "\n",
    "For this notebook, we are going to use [`unsloth/Qwen3-14B-GGUF`](https://huggingface.co/unsloth/Qwen3-14B-GGUF). This is a larger and more modern LLM compared to [`meta-llama/Llama-3.1-8B-Instruct`](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), which should lead to better results. We are also going to use the Chat Completion API, which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed266c25-a5ae-4cff-b374-87fc37cf7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Mode(Enum):\n",
    "    \"\"\"Response API not implemented since currently not supported by vLLM.\"\"\"\n",
    "\n",
    "    COMPLETION = \"v1/completions\"\n",
    "    CHAT_COMPLETION = \"v1/chat/completions\"\n",
    "\n",
    "\n",
    "def answer(\n",
    "    prompt: str, guided_json=None, mode: Mode = Mode.CHAT_COMPLETION, **kwargs\n",
    "):\n",
    "    \"\"\"Helper function to prompt LLM.\n",
    "\n",
    "    Handles both Chat Completion and Completion API format.\n",
    "    kwargs can override config values like max_tokens, temperature, etc.\n",
    "    \"\"\"\n",
    "    # Overwrite the default API mode and any other kwargs\n",
    "    config_overrides = {\"mode\": mode.value, **kwargs}\n",
    "    config = AddonConfig.create_nullable(config_overrides)\n",
    "\n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        prompt_param_name = \"messages\"\n",
    "        prompt_param_value = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    else:\n",
    "        prompt_param_name = \"prompt\"\n",
    "        prompt_param_value = prompt\n",
    "\n",
    "    payload = {\n",
    "        \"model\": config.model_name,\n",
    "        prompt_param_name: prompt_param_value,\n",
    "        \"max_tokens\": config.max_tokens,\n",
    "        \"temperature\": config.temperature,\n",
    "        \"top_p\": config.top_p,\n",
    "        \"min_p\": config.min_p,\n",
    "        \"top_k\": config.top_k,\n",
    "    }\n",
    "\n",
    "    if guided_json is not None:\n",
    "        payload[\"guided_json\"] = guided_json\n",
    "\n",
    "    response = requests.post(config.url, json=payload)\n",
    "    print(response)\n",
    "\n",
    "    # Check for errors\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error response: {response.text}\")\n",
    "        raise Exception(\n",
    "            f\"Server returned {response.status_code}: {response.text}\"\n",
    "        )\n",
    "\n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        reasoning_content = response.json()[\"choices\"][0][\"message\"][\n",
    "            \"reasoning_content\"\n",
    "        ]\n",
    "        content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(f\"Content: {content.replace('<think>\\n\\n</think>\\n\\n', '')}\")\n",
    "        print(f\"Reasoning: {reasoning_content}\")\n",
    "        return (content, reasoning_content)\n",
    "\n",
    "    if mode == Mode.COMPLETION:\n",
    "        content = response.json()[\"choices\"][0][\"text\"]\n",
    "        print(f\"Content: {content}\")\n",
    "        return (content, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2acce-c7a3-4a2e-94d9-dc27aac943d5",
   "metadata": {},
   "source": [
    "### Completions vs Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efc5e84-5fb7-4963-9e13-3626e4b3c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "ciao\n",
      "Reasoning: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    prompt=\"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.CHAT_COMPLETION,\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94104257-3345-419d-a15c-c899890dcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Okay, the user is asking for the Italian word for '\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION,\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d079c9f-e977-4ab4-9086-7fd353e8fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "# \n",
      "\n",
      "Hello\n",
      "\n",
      "# \n",
      "\n",
      "ciao\n",
      "\n",
      "# \n",
      "\n",
      "ciao\n",
      "\n",
      "# \n",
      "\n",
      "ciao\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION,\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db56374-238e-4da5-8239-eb4ddb729c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "# The user is asking for the Italian word for 'hello'. I need to provide the correct\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION,\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b7e3c-933d-48c3-b2b9-ec80e0237402",
   "metadata": {},
   "source": [
    "For Qwen 3, the Chat Completions API appears to work much better. In our simple test case, when using the Completions API, Qwen 3 tends to enter a pattern where it repeats itself until reaching the `max_tokens` limit.\n",
    "\n",
    "We also notice that, despite asking Qwen 3 not to output \"thinking tokens\", it still does that in the `content` field. The thinking tokens are also returned both in the `content` and `reasoning` fields and do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb2684-439a-4a96-90dd-266dbfee8d38",
   "metadata": {},
   "source": [
    "## Select a few flashcards for our offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3f1b-7b16-4c22-a8ba-c78109a2fa04",
   "metadata": {},
   "source": [
    "We have everything we need now to tell the LLM to make some changes to our Anki flashcards.\n",
    "\n",
    "Let's pull a few note currently marked for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4449c78a-c525-43b3-96f4-cf665617105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_id = col.decks.current()[\"id\"]\n",
    "query = f\"did:{deck_id}\"\n",
    "note_ids = col.find_notes(query)\n",
    "\n",
    "NUM_NOTES_NEEDED = 10\n",
    "\n",
    "flagged_notes = []\n",
    "for note_id in note_ids:\n",
    "    if is_note_marked_for_review(col, note_id):\n",
    "        note = col.get_note(note_id)\n",
    "        flagged_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cad38b4-78b1-4790-8e4b-b2a77e763c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flagged notes: 284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of flagged notes: {len(flagged_notes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d6db93-fc16-43bb-b348-1e49e7df1e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)\n"
     ]
    }
   ],
   "source": [
    "from addon.application.services.formatter_service import AnkiNoteAdapter\n",
    "\n",
    "addon_note = AnkiNoteAdapter.to_addon_note(flagged_notes[0])\n",
    "print(addon_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af649819-1267-4ea3-910a-aac5265734aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\\n\\nAddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\\n\\n{addon_note}\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec669fd-203e-439c-8f94-75b9428c3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "The flashcard is concise and accurate, but here's an improved version with minor refinements for clarity and completeness:\n",
      "\n",
      "**Front:**  \n",
      "\"Can bagging be parallelized?\"  \n",
      "\n",
      "**Back:**  \n",
      "\"Yes. Bagging (Bootstrap Aggregating) involves training independent models on bootstrap samples, which can be done in parallel due to their independence.\"  \n",
      "\n",
      "**Improvements:**  \n",
      "1. **Simplified phrasing:** \"Parallelized\" is slightly more technical and concise than \"performed in parallel.\"  \n",
      "2. **Added context:** Explains why bagging is parallelizable (independence of models and bootstrap samples) without overcomplicating the answer.  \n",
      "3. **Maintained atomicity:** The question and answer remain focused on a single concept.  \n",
      "\n",
      "**Optional:** If the deck allows, add a follow-up flashcard:  \n",
      "**Front:** \"Why is bagging suitable for parallel processing?\"  \n",
      "**Back:** \"Each model is trained independently on different bootstrap samples, eliminating the need for sequential execution.\"  \n",
      "\n",
      "This ensures the core concept is clear while providing depth for deeper understanding.\n",
      "Reasoning: \n",
      "Okay, let's take a look at this flashcard. The question is \"Can bagging be performed in parallel?\" and the answer is \"Yes.\" The user wants to know how to improve it, keeping in mind that flashcards should be atomic, concise, and accurate.\n",
      "\n",
      "First, I need to check if the question is atomic. \"Can bagging be performed in parallel?\" is a yes/no question, which is straightforward. However, maybe it's better to rephrase it to make it more specific. For example, \"Is bagging suitable for parallel processing?\" might be more precise, but the original is already clear.\n",
      "\n",
      "Next, conciseness. The answer is \"Yes,\" which is concise. But maybe adding a brief explanation would help, especially if the user is studying for an exam or needs more context. However, the original instruction says to keep it concise, so maybe that's not necessary. But the user might be okay with a slightly more detailed answer as long as it's still concise.\n",
      "\n",
      "Accuracy is important. Bagging, like in Random Forests, involves training multiple models on different subsets of data. Since each model is trained independently, bagging can indeed be done in parallel. So the answer \"Yes\" is accurate. However, the original answer is very brief. Maybe expanding it a bit to mention that each model is trained on a bootstrap sample and that this independence allows parallel processing would be more informative without being too lengthy.\n",
      "\n",
      "Also, the tags are set to 'ml', which is correct. The notetype is basic, which is appropriate for a yes/no question. The deck name is None, which might be okay if it's part of a larger deck, but the user might want to specify a deck name for better organization.\n",
      "\n",
      "Another point: the question is about the possibility of parallel processing. Maybe the answer could explain why it's possible, like the independence of each model's training. However, the user wants the flashcard to be atomic and concise, so adding that might make it too long. But in some cases, a bit more context can help retention without sacrificing conciseness.\n",
      "\n",
      "So, possible improvements could be:\n",
      "\n",
      "1. Rephrase the question to be more specific or precise.\n",
      "2. Add a brief explanation to the answer while keeping it concise.\n",
      "3. Ensure the tags and deck name are correctly set if needed.\n",
      "4. Check if the answer is accurate and if any additional context is necessary.\n",
      "\n",
      "But the user's original flashcard is already quite simple. Maybe the main improvement is to make the answer slightly more informative. For example, changing the back to \"Yes, because each model in bagging is trained independently on different bootstrap samples, allowing parallel processing.\" However, this might be too long for a flashcard. Alternatively, keep the answer as \"Yes\" but add a note in the tags or comments for elaboration, but the system might not support that.\n",
      "\n",
      "Alternatively, split into two flashcards: one for the question and answer, and another for the explanation. But the user might prefer a single card.\n",
      "\n",
      "Considering all that, the best improvement might be to rephrase the question for clarity and ensure the answer is accurate and concise. The original answer is correct, but adding a reason could be helpful. However, if the user strictly wants atomic and concise, then the original is fine. Maybe the user just wants the question to be more precise. For example, \"Can bagging algorithms be parallelized?\" instead of \"Can bagging be performed in parallel?\" but that's a minor change.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    prompt=prompt, mode=Mode.CHAT_COMPLETION, max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bc6f5-2c81-4291-af65-371c76c4ee56",
   "metadata": {},
   "source": [
    "The model is doing a good job, at least on this flashcard. However, we probably need some structured output/constrained decoding to ensure the LLM respects a predefined format. This will make extracting the suggestions more easier.\n",
    "\n",
    "Let's reuse the `pydantic` schema we have already defined for the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12f4cd6-cb99-4f58-b27f-5857a00d474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: {\"front\": \"Can the training of individual models in bagging be parallelized?\", \"back\": \"Yes, because each model is trained on independent bootstrap samples.\", \"tags\": [\"ml\"]}\n",
      "Reasoning: \n",
      "Okay, let's see. The user provided a flashcard about whether bagging can be performed in parallel. The front is the question, and the back is the answer. The user wants to know how to improve this flashcard, keeping in mind that flashcards should be atomic, concise, and accurate.\n",
      "\n",
      "First, I need to recall what bagging is. Bagging, or Bootstrap Aggregating, is an ensemble method where multiple models are trained on different subsets of the data, and their predictions are combined. Examples include Random Forests. Now, can this be done in parallel?\n",
      "\n",
      "Well, since each model in bagging is trained on different data samples, they are independent of each other. That means you can train each model in parallel. However, the answer given is \"Yes,\" which is correct. But maybe the flashcard is too simplistic. Let me think about possible improvements.\n",
      "\n",
      "The original flashcard is straightforward, but maybe it lacks context. For example, it doesn't mention that the models are independent, which is why parallelization is possible. Also, it might be helpful to note that while the training can be parallelized, the aggregation step (like averaging or voting) is usually done sequentially. But the question is specifically about performing bagging, which refers to the training process. So the answer is correct, but maybe the explanation could be more detailed.\n",
      "\n",
      "However, the user mentioned that flashcards should be atomic and concise. So adding more details might make it less concise. But perhaps the current answer is too vague. The answer \"Yes\" is correct, but maybe it's better to specify that the individual models can be trained in parallel, not the entire bagging process. Or mention that the parallelization is possible due to the independence of the models.\n",
      "\n",
      "Alternatively, the question could be rephrased to be more precise. For example, \"Can the training of models in bagging be parallelized?\" instead of \"Can bagging be performed in parallel?\" because \"bagging\" as a process might include both training and aggregation steps. But the original question is a bit ambiguous. However, the answer is correct in the context of the training phase.\n",
      "\n",
      "Another point: the original flashcard doesn't have any explanation. Maybe adding a brief note on the back about why it's possible would help, but again, that might make it less concise. The user wants it to be atomic and concise, so perhaps keeping the answer as \"Yes\" is better, but maybe the question can be made more precise.\n",
      "\n",
      "So, possible improvements could be:\n",
      "\n",
      "1. Make the question more specific, like \"Can the individual models in bagging be trained in parallel?\"\n",
      "2. Clarify that the answer refers to the training process, not the entire bagging process.\n",
      "3. Ensure that the answer is accurate and not misleading. Since the models are independent, yes, they can be trained in parallel.\n",
      "\n",
      "But the original answer is technically correct. However, maybe the user is looking for a more detailed explanation. However, given the constraints of a flashcard, it's better to keep it concise. So perhaps the original is okay, but maybe the question can be rephrased for clarity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from addon.infrastructure.llm.schemas import AddonNoteChanges\n",
    "\n",
    "content, reasoning_content = answer(\n",
    "    prompt=prompt,\n",
    "    guided_json=AddonNoteChanges.model_json_schema(),\n",
    "    mode=Mode.CHAT_COMPLETION,\n",
    "    max_tokens=1_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60e007-968e-4e19-a75d-2ac241a44253",
   "metadata": {},
   "source": [
    "Not bad! Let's compare it with the current version we have in the `main` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a3de30-f1f3-4a53-8859-ccfa8bb60d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddonNote(front='Can bagging be parallelized?', back='\"Yes\"', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from addon.application.services.formatter_service import NoteFormatter\n",
    "from addon.infrastructure.external_services.openai import OpenAIClient\n",
    "\n",
    "config = AddonConfig.create_nullable()\n",
    "openai = OpenAIClient.create(config)\n",
    "formatter = NoteFormatter(openai)\n",
    "\n",
    "formatter.format(note=addon_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa057295-7dc6-492b-9f96-8181f468fbda",
   "metadata": {},
   "source": [
    "The current version in productin basically does not make any change to the original flashcard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6541e02d-ceb4-4629-954b-6d65f9ea8fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addon_note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05200894-9109-42c0-b4f8-825cda86c8e7",
   "metadata": {},
   "source": [
    "Let's try to let the LLM reason, while still perform constrained decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3efe11-b155-4e4c-b7b3-5c05ef2725b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: {\"front\": \"Can bagging be parallelized?\", \"back\": \"Yes, because individual models are trained on independent bootstrap samples.\", \"tags\": [\"ml\", \"bagging\"]}\n",
      "Reasoning: \n",
      "Okay, let's take a look at this flashcard. The question is \"Can bagging be performed in parallel?\" and the answer is \"Yes.\" Hmm, I need to figure out how to improve this. The user mentioned that flashcards should be atomic, concise, and accurate.\n",
      "\n",
      "First, \"atomic\" probably means each flashcard should cover one concept. The question here is about whether bagging can be parallelized. The answer is \"Yes,\" but maybe that's too vague. What's the reasoning behind that? Bagging, like in Random Forests, uses bootstrap samples and each tree is built independently. So the answer could be more detailed. But wait, the user said concise. So maybe adding a bit more context without being too wordy.\n",
      "\n",
      "Also, the tags are 'ml', which is good. The note type is basic, so maybe it's okay. But the answer is just \"Yes.\" Maybe the answer should explain why, but the user wants it concise. Maybe the answer is okay as is, but the question could be phrased better. Like, \"Is bagging inherently parallelizable?\" or \"Can bagging algorithms be parallelized?\" That might be more precise.\n",
      "\n",
      "Another thing: the front is a question, but maybe it's better to use a statement. Like \"Bagging can be performed in parallel. Why?\" But that might not be standard for flashcards. Alternatively, keeping the question as is but ensuring the answer is accurate. Wait, is bagging always parallelizable? Well, since each bootstrap sample is independent, the individual models (like trees in Random Forest) can be trained in parallel. So the answer is yes, but maybe the answer should mention that the individual models are independent. But again, the user wants concise. So maybe the answer is okay, but the question could be more precise.\n",
      "\n",
      "Also, the deck name is None, which might be okay if it's part of a larger set. But maybe adding a deck name would help categorization. However, the user didn't mention that, so maybe not necessary here.\n",
      "\n",
      "So, possible improvements: rephrase the question for clarity, ensure the answer is accurate and concise, maybe add a note about independence of models. But since the user wants atomic and concise, perhaps just adjusting the question slightly and keeping the answer as is. Alternatively, if the answer is too brief, maybe expanding it a bit but keeping it short. For example, \"Yes, because individual models in bagging are trained on independent samples.\" But that's a bit longer. However, the original answer is just \"Yes,\" which might be too brief. But the user might prefer it that way for quick recall.\n",
      "\n",
      "Another angle: check if the statement is accurate. Bagging involves creating multiple bootstrap samples and training models on each. Since each model is trained on separate data, they can be parallelized. So the answer is correct. But maybe the answer should be more specific, like \"Yes, as each bootstrap sample is independent, allowing parallel training of models.\" But again, the user wants it concise. So maybe the original is okay, but the question could be more precise.\n",
      "\n",
      "So, the main improvements would be:\n",
      "\n",
      "1. Make the question more precise, perhaps by specifying \"algorithms\" or \"process.\"\n",
      "2. Ensure the answer is accurate and maybe slightly more informative without being too lengthy.\n",
      "3. Check if the tags are sufficient (ml is good, but maybe add 'bagging' or 'random-forest' as tags for better categorization).\n",
      "\n",
      "But the user might not want to add tags, just improve the content. So the final answer would be adjusting the question and maybe the answer slightly for clarity and accuracy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from addon.infrastructure.llm.schemas import AddonNoteChanges\n",
    "\n",
    "prompt = f\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate. /think \\n\\n{addon_note}\"\n",
    "content, reasoning_content = answer(\n",
    "    prompt=prompt,\n",
    "    guided_json=AddonNoteChanges.model_json_schema(),\n",
    "    mode=Mode.CHAT_COMPLETION,\n",
    "    max_tokens=1_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c735cfe6-db80-45d6-89d3-72acf9cbaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01900f2a-bea3-45ec-b277-8c8166e607f1",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Check Qwen 3's instruction following capabilities with Completions and Chat Completions API\n",
    "- [x] Simple prompt\n",
    "- [x] Simple prompt + Constrained Decoding\n",
    "- [x] Simple prompt + Constrained Decoding + Reasoning\n",
    "- [ ] Simple prompt + Chain of Thought\n",
    "- [ ] Two-step process (critique → refine)\n",
    "- [ ] Agent\n",
    "- [ ] Check if we have a class that we can use to read the collection from the hard disk and convert it to `AddonNote` instead of `Note`. In that way we can operate with domain objects instead of external dependencies. The same class should be used to convert the `AddonNote` back to `Note` (so maybe we should keep track of the `note_id`\n",
    "- [ ] Once we have that class, we should update the rest of the codebase accordingly (e.g., note formatter, note counter, etc.)\n",
    "- [ ] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c8e77-6198-4a26-b303-65546c7249cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
