{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88561fd4-aa53-4912-9301-e7f887f28ddd",
   "metadata": {},
   "source": [
    "# Formatting Anki Flashcards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8e3c6-7ebb-48b9-81b5-ca64b56205b6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c977e-ff6e-452f-9d7d-81b53569213f",
   "metadata": {},
   "source": [
    "In this notebook, we are exploring the best way to prompt an LLM to improve the formatting of Anki flashcards. \n",
    "\n",
    "Approaches we are including:\n",
    "1. Simple prompt\n",
    "2. Simple prompt + Chain of Thought\n",
    "3. Two-step process (critique → refine)\n",
    "4. Agent\n",
    "\n",
    "For this notebook, we are going to use [`unsloth/Qwen3-14B-GGUF`](https://huggingface.co/unsloth/Qwen3-14B-GGUF). This is a larger and more modern LLM compared to [`meta-llama/Llama-3.1-8B-Instruct`](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), which should lead to better results. We chose `Qwen3-14B` because it's a good-performing open-weight LLM that fits in 24GB of VRAM and delivers acceptable inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b40c65b-b584-47c7-9094-24187a434858",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471e4473-274b-491a-ae26-fa61d07b715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from anki.collection import Collection\n",
    "\n",
    "from addon.application.use_cases.note_counter import is_note_marked_for_review\n",
    "from addon.infrastructure.configuration.settings import AddonConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a631ea3e-b3b2-471d-8395-30c898b07a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes: 3362\n",
      "Number of cards: 3498\n"
     ]
    }
   ],
   "source": [
    "# Open an existing collection\n",
    "col = Collection(\"/home/gianluca/.local/share/Anki2/User 1/collection.anki2\")\n",
    "\n",
    "# Do something with the collection\n",
    "print(f\"Number of notes: {col.note_count()}\")\n",
    "print(f\"Number of cards: {col.card_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859fd3-c78b-44f3-b03c-d4d565446970",
   "metadata": {},
   "source": [
    "### Connect to Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e1b64c-e5e1-4963-b56b-71d2ab474944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "def display_addon_note(addon_note: AddonNote) -> None:\n",
    "    print(f\"Front: {addon_note.front}\\nBack: {addon_note.back}\\nTags: {addon_note.tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83217ea4-94f0-4c4f-b3f2-e2e2a5d6953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addon.infrastructure.external_services.openai import OpenAIClient\n",
    "\n",
    "def answer(\n",
    "    input: str | list[dict],\n",
    "    guided_json = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Helper function to prompt LLM.\n",
    "    \n",
    "    input: Either a string (completions API) or list of message dicts (chat completion)\n",
    "    kwargs: Can override config values like max_tokens, temperature, etc.\n",
    "    \"\"\"\n",
    "    # Create config with any overrides\n",
    "    config = AddonConfig.create_nullable(kwargs)\n",
    "    \n",
    "    # Create OpenAI client\n",
    "    client = OpenAIClient.create(config)\n",
    "    \n",
    "    # Build kwargs for the run method\n",
    "    run_kwargs = {}\n",
    "    if guided_json is not None:\n",
    "        run_kwargs[\"guided_json\"] = guided_json\n",
    "    \n",
    "    # Run the inference\n",
    "    content = client.run(input, **run_kwargs)\n",
    "    \n",
    "    # Clean thinking tokens and return\n",
    "    cleaned_content = content.replace('<think>\\n\\n</think>\\n\\n', '')\n",
    "    print(f\"Content: {cleaned_content}\")\n",
    "\n",
    "    if guided_json is not None:\n",
    "        suggested_changes = AddonNoteChanges.model_validate_json(content)\n",
    "        return (suggested_changes, None)\n",
    "\n",
    "    # Note: OpenAIClient doesn't currently return reasoning_content separately\n",
    "    # so we return None for backward compatibility\n",
    "    return (content, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2acce-c7a3-4a2e-94d9-dc27aac943d5",
   "metadata": {},
   "source": [
    "## Completions vs. Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7efc5e84-5fb7-4963-9e13-3626e4b3c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "</think>\n",
      "\n",
      "ciao\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    input=\"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=\"v1/completions\",\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94104257-3345-419d-a15c-c899890dcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    input=\"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=\"v1/completions\",\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d079c9f-e977-4ab4-9086-7fd353e8fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "\n",
      "# The user is asking for the Italian word for 'hello'. I need to provide the correct\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    input=\"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=\"v1/completions\",\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db56374-238e-4da5-8239-eb4ddb729c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "\n",
      "ciao\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    input=[{\"role\": \"system\", \"content\": \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\"}],\n",
    "    mode=\"v1/chat/completions\",\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b7e3c-933d-48c3-b2b9-ec80e0237402",
   "metadata": {},
   "source": [
    "For Qwen 3, the Chat Completions API appears to work much better. In our simple test case, when using the Completions API, Qwen 3 tends to enter a pattern where it repeats itself until reaching the `max_tokens` limit.\n",
    "\n",
    "We also notice that, despite asking Qwen 3 not to output \"thinking tokens\", it still does that in the `content` field. The thinking tokens are also returned both in the `content` and `reasoning` fields and do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb2684-439a-4a96-90dd-266dbfee8d38",
   "metadata": {},
   "source": [
    "## Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3f1b-7b16-4c22-a8ba-c78109a2fa04",
   "metadata": {},
   "source": [
    "We have everything we need now to tell the LLM to make some changes to our Anki flashcards.\n",
    "\n",
    "Let's pull a few note currently marked for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4449c78a-c525-43b3-96f4-cf665617105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_id = col.decks.current()[\"id\"]\n",
    "query = f\"did:{deck_id}\"\n",
    "note_ids = col.find_notes(query)\n",
    "\n",
    "NUM_NOTES_NEEDED = 10\n",
    "\n",
    "flagged_notes = []\n",
    "for note_id in note_ids:\n",
    "    if is_note_marked_for_review(col, note_id):\n",
    "        note = col.get_note(note_id)\n",
    "        flagged_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cad38b4-78b1-4790-8e4b-b2a77e763c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flagged notes: 288\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of flagged notes: {len(flagged_notes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d6db93-fc16-43bb-b348-1e49e7df1e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: Most important thing to accelerate memory lookups\n",
      "Back: Data availability on GPU memory<br><br><img src=\"Screenshot from 2022-02-07 11-31-00.png\"><br><br>Ref.: <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/\">https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/</a>\n",
      "Tags: ['recsys']\n"
     ]
    }
   ],
   "source": [
    "from addon.application.services.formatter_service import AnkiNoteAdapter\n",
    "\n",
    "addon_note = AnkiNoteAdapter.to_addon_note(flagged_notes[0])\n",
    "display_addon_note(addon_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2096be-c2ff-4e98-b758-66c72a4deb4a",
   "metadata": {},
   "source": [
    "## Simple Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af649819-1267-4ea3-910a-aac5265734aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\n",
      "\n",
      "AddonNote(front='Most important thing to accelerate memory lookups', back='Data availability on GPU memory<br><br><img src=\"Screenshot from 2022-02-07 11-31-00.png\"><br><br>Ref.: <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/\">https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/</a>', guid='OM_`a7R~Dp', tags=['recsys'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\\n\\n{addon_note}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec669fd-203e-439c-8f94-75b9428c3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "\n",
      "Here’s an improved version of the flashcard, focusing on atomicity, conciseness, and accuracy:\n",
      "\n",
      "---\n",
      "\n",
      "**Front:**  \n",
      "\"Key factor for accelerating GPU memory lookups\"  \n",
      "\n",
      "**Back:**  \n",
      "\"Data must be resident in GPU memory (not CPU memory) to enable fast access during computation.  \n",
      "Ref.: [NVIDIA GTC 2021 Talk](https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/)\"  \n",
      "\n",
      "---\n",
      "\n",
      "**Improvements made:**  \n",
      "1. **Front:** Simplified and clarified the question to focus on the specific context (GPU memory lookups).  \n",
      "2. **Back:** Removed the image (non-essential for memorization) and replaced it with a concise explanation of the core concept.  \n",
      "3. **Reference:** Retained the citation but formatted it as a direct link for clarity.  \n",
      "4. **Conciseness:** Trimmed extraneous text to focus on the atomic fact: *data residency in GPU memory* as the critical enabler.  \n",
      "\n",
      "This version adheres to flashcard best practices by prioritizing clarity, brevity, and actionable knowledge.\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    input=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e3898-6354-4201-8a79-d7b1083a917f",
   "metadata": {},
   "source": [
    "Despite the very simple prompt, on this flashcard, the model is doing a good job. However, we probably need some structured output/constrained decoding to ensure the LLM respects a predefined format. This will make extracting the suggestions more easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aecf0b-8df2-4130-a6f8-0577444651c8",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd8e45-0a03-4018-8e1b-7e2fe983249c",
   "metadata": {},
   "source": [
    "Let's reuse the `pydantic` schema we have already defined for the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12f4cd6-cb99-4f58-b27f-5857a00d474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: {\n",
      "\n",
      "\"front\": \"What is the most important factor to accelerate memory lookups in GPU-based systems?\",  \n",
      "\"back\": \"Data availability on GPU memory. Ensuring data is resident in GPU memory reduces latency and accelerates access during computations.\\n\\nRef.: https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/\",  \n",
      "\"tags\": [\"recsys\", \"gpu-optimization\"]  \n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from addon.infrastructure.llm.schemas import AddonNoteChanges\n",
    "\n",
    "content, reasoning_content = answer(\n",
    "    input=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    guided_json=AddonNoteChanges.model_json_schema(),\n",
    "    max_tokens=1_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897ca904-3401-4ad1-8462-4d6f82648dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: What is the most important factor to accelerate memory lookups in GPU-based systems?\n",
      "Back: Data availability on GPU memory. Ensuring data is resident in GPU memory reduces latency and accelerates access during computations.\n",
      "\n",
      "Ref.: https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/\n",
      "Tags: ['recsys', 'gpu-optimization']\n"
     ]
    }
   ],
   "source": [
    "display_addon_note(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a035c9b-814b-4429-b2ba-8e4a53f283dd",
   "metadata": {},
   "source": [
    "## Chat Completions + Structured Output + Old Prompt + Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bdc42-053a-45b2-8f02-5c78565cd82e",
   "metadata": {},
   "source": [
    "Let's compare it with the current version we have in the `main` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca9772ad-c235-4c55-9feb-5ca7247d899b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: ./Qwen3-14B-Q8_0.gguf\n",
      "temperature: 0.7\n",
      "max_tokens: 5000\n",
      "top_p: 0.8\n",
      "top_k: 20\n",
      "min_p: 0.0\n"
     ]
    }
   ],
   "source": [
    "from addon.application.services.formatter_service import NoteFormatter\n",
    "from addon.infrastructure.external_services.openai import OpenAIClient\n",
    "\n",
    "config = AddonConfig.create_nullable({\n",
    "    \"mode\": \"v1/chat/completions\",\n",
    "    \"url\": \"http://iamgianluca.ddns.net:8080/v1/completions\",\n",
    "    \"max_tokens\": 5_000,  # migh fail from time to time, depending \n",
    "})\n",
    "for k, v in config.__dict__.items():\n",
    "    if k == \"url\":  # hide inference server url\n",
    "        continue\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "457484b5-6b78-48d8-bc5f-7611ba1e891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: Accelerate memory lookups: Key factor\n",
      "Back: Data availability on GPU memory\n",
      "\n",
      "<img src=\"Screenshot from 2022-02-07 11-31-00.png\">\n",
      "\n",
      "Ref.: <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/\">https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/</a>\n",
      "Tags: ['recsys']\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAIClient.create(config)\n",
    "formatter = NoteFormatter(openai)\n",
    "new_note = formatter.format(note=addon_note)\n",
    "\n",
    "display_addon_note(new_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa057295-7dc6-492b-9f96-8181f468fbda",
   "metadata": {},
   "source": [
    "The current version in production, only barely change to the original flashcard, and focuses mostly on formatting. This is likely due to the highly specific prompt. A shorter and more generic prompt, in combination with reasoning, could lead to better results.\n",
    "\n",
    "For reference, below is the original note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc8ee6f2-d076-496f-ba62-0e747285b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL NOTE\n",
      "\n",
      "Front: Most important thing to accelerate memory lookups\n",
      "Back: Data availability on GPU memory<br><br><img src=\"Screenshot from 2022-02-07 11-31-00.png\"><br><br>Ref.: <a href=\"https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/\">https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/</a>\n",
      "Tags: ['recsys']\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL NOTE\\n\")\n",
    "display_addon_note(addon_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61988d20-1295-4ec0-92e5-9119d4a76dff",
   "metadata": {},
   "source": [
    "## Reasoning + Structured Output + Simple Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05200894-9109-42c0-b4f8-825cda86c8e7",
   "metadata": {},
   "source": [
    "Let's try to let the LLM reason, while still perform constrained decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c3efe11-b155-4e4c-b7b3-5c05ef2725b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: {\n",
      "\n",
      "\"front\": \"What is the most critical factor for accelerating memory lookups in GPU computing?\",  \n",
      "\"back\": \"Data availability on GPU memory. (Visual aid: Screenshot from NVIDIA GTC Fall 2021 session on GPU memory optimization)\\n\\nReference: [NVIDIA GTC Fall 2021 Session](https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/)\",  \n",
      "\"tags\": [\"recsys\", \"GPU\", \"memory-optimization\"]  \n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from addon.infrastructure.llm.schemas import AddonNoteChanges\n",
    "\n",
    "prompt = f\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate. /think \\n\\n{addon_note}\"\n",
    "content, reasoning_content = answer(\n",
    "    input=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    guided_json=AddonNoteChanges.model_json_schema(),\n",
    "    max_tokens=1_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f637019-0250-4aef-9098-7e46553888cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: What is the most critical factor for accelerating memory lookups in GPU computing?\n",
      "Back: Data availability on GPU memory. (Visual aid: Screenshot from NVIDIA GTC Fall 2021 session on GPU memory optimization)\n",
      "\n",
      "Reference: [NVIDIA GTC Fall 2021 Session](https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31230/)\n",
      "Tags: ['recsys', 'GPU', 'memory-optimization']\n"
     ]
    }
   ],
   "source": [
    "display_addon_note(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c735cfe6-db80-45d6-89d3-72acf9cbaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01900f2a-bea3-45ec-b277-8c8166e607f1",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Check Qwen 3's instruction following capabilities with Completions and Chat Completions API\n",
    "- [x] Simple prompt\n",
    "- [x] Simple prompt + Constrained Decoding\n",
    "- [x] Simple prompt + Constrained Decoding + Reasoning\n",
    "- [ ] Simple prompt + Chain of Thought\n",
    "- [ ] Cleaner representation of existing addon note in the prompt passed to the LLM\n",
    "- [ ] Pass note to change as `user` instead of `system` role\n",
    "- [ ] Two-step process (critique → refine)\n",
    "- [ ] Agent\n",
    "- [ ] Check if we have a class that we can use to read the collection from the hard disk and convert it to `AddonNote` instead of `Note`. In that way we can operate with domain objects instead of external dependencies. The same class should be used to convert the `AddonNote` back to `Note` (so maybe we should keep track of the `note_id`\n",
    "- [ ] Once we have that class, we should update the rest of the codebase accordingly (e.g., note formatter, note counter, etc.)\n",
    "- [ ] ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
