{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88561fd4-aa53-4912-9301-e7f887f28ddd",
   "metadata": {},
   "source": [
    "# Formatting Anki Flashcards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16655e-0278-4a9e-9d3e-414bdeeae376",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this notebook, we are exploring the best way to prompt an LLM to improve the formatting of Anki flashcards. \n",
    "\n",
    "Approaches we are including:\n",
    "1. Simple prompt\n",
    "2. Simple prompt + Chain of Thought\n",
    "3. Two-step process (critique → refine)\n",
    "4. Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b40c65b-b584-47c7-9094-24187a434858",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471e4473-274b-491a-ae26-fa61d07b715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from anki.collection import Collection\n",
    "\n",
    "from addon.application.use_cases.note_counter import is_note_marked_for_review\n",
    "from addon.infrastructure.configuration.settings import AddonConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a631ea3e-b3b2-471d-8395-30c898b07a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes: 3362\n",
      "Number of cards: 3498\n"
     ]
    }
   ],
   "source": [
    "# Open an existing collection\n",
    "col = Collection(\"/home/gianluca/.local/share/Anki2/User 1/collection.anki2\")\n",
    "\n",
    "# Do something with the collection\n",
    "print(f\"Number of notes: {col.note_count()}\")\n",
    "print(f\"Number of cards: {col.card_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859fd3-c78b-44f3-b03c-d4d565446970",
   "metadata": {},
   "source": [
    "### Connect to Inference Server\n",
    "\n",
    "For this notebook, we are going to use [`unsloth/Qwen3-14B-GGUF`](https://huggingface.co/unsloth/Qwen3-14B-GGUF). This is a larger and more modern LLM compared to [`meta-llama/Llama-3.1-8B-Instruct`](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), which should lead to better results. We are also going to use the Chat Completion API, which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed266c25-a5ae-4cff-b374-87fc37cf7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Mode(Enum):\n",
    "    \"\"\"Response API not implemented since currently not supported by vLLM.\"\"\"\n",
    "\n",
    "    COMPLETION = \"v1/completions\"\n",
    "    CHAT_COMPLETION = \"v1/chat/completions\"\n",
    "\n",
    "\n",
    "def answer(\n",
    "    prompt: str,\n",
    "    mode: Mode = Mode.CHAT_COMPLETION,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Helper function to prompt LLM.\n",
    "\n",
    "    Handles both Chat Completion and Completion API format.\n",
    "    kwargs can override config values like max_tokens, temperature, etc.\n",
    "    \"\"\"\n",
    "    # Overwrite the default API mode and any other kwargs\n",
    "    config_overrides = {\"mode\": mode.value, **kwargs}\n",
    "    config = AddonConfig.create_nullable(config_overrides)\n",
    "    \n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        prompt_param_name = \"messages\"\n",
    "        prompt_param_value = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    else:\n",
    "        prompt_param_name = \"prompt\"\n",
    "        prompt_param_value = prompt\n",
    "\n",
    "    payload = {\n",
    "        \"model\": config.model_name,\n",
    "        prompt_param_name: prompt_param_value,\n",
    "        \"max_tokens\": config.max_tokens,\n",
    "        \"temperature\": config.temperature,\n",
    "        \"top_p\": config.top_p,\n",
    "        \"min_p\": config.min_p,\n",
    "        \"top_k\": config.top_k,\n",
    "    }\n",
    "\n",
    "    response = requests.post(config.url, json=payload)\n",
    "    print(response)\n",
    "    \n",
    "    # Check for errors\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error response: {response.text}\")\n",
    "        raise Exception(f\"Server returned {response.status_code}: {response.text}\")\n",
    "\n",
    "    if mode == Mode.CHAT_COMPLETION:\n",
    "        reasoning_content = response.json()[\"choices\"][0][\"message\"][\n",
    "            \"reasoning_content\"\n",
    "        ]\n",
    "        content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(f\"Content: {content.replace('<think>\\n\\n</think>\\n\\n', '')}\")\n",
    "        print(f\"Reasoning: {reasoning_content}\")\n",
    "        return (content, reasoning_content)\n",
    "\n",
    "    if mode == Mode.COMPLETION:\n",
    "        content = response.json()[\"choices\"][0][\"text\"]\n",
    "        print(f\"Content: {content}\")\n",
    "        return (content, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2acce-c7a3-4a2e-94d9-dc27aac943d5",
   "metadata": {},
   "source": [
    "### Completions vs Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efc5e84-5fb7-4963-9e13-3626e4b3c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: ciao\n",
      "Reasoning: None\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    prompt=\"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.CHAT_COMPLETION, \n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94104257-3345-419d-a15c-c899890dcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "# The user is asking for the Italian word for 'hello'. I need to provide the correct\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION, \n",
    "    max_tokens=20\n",
    ",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d079c9f-e977-4ab4-9086-7fd353e8fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION, \n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db56374-238e-4da5-8239-eb4ddb729c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: \n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "ciao\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Assistant:\n",
      "\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    \"Respond only with one word, lowercase, without punctuation. What is the Italian word for 'hello'? /no_think\",\n",
    "    mode=Mode.COMPLETION, \n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b7e3c-933d-48c3-b2b9-ec80e0237402",
   "metadata": {},
   "source": [
    "For Qwen 3, the Chat Completions API appears to work much better. In our simple test case, when using the Completions API, Qwen 3 tends to enter a pattern where it repeats itself until reaching the `max_tokens` limit.\n",
    "\n",
    "We also notice that, despite asking Qwen 3 not to output \"thinking tokens\", it still does that in the `content` field. The thinking tokens are also returned both in the `content` and `reasoning` fields and do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb2684-439a-4a96-90dd-266dbfee8d38",
   "metadata": {},
   "source": [
    "## Select a few flashcards for our offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3f1b-7b16-4c22-a8ba-c78109a2fa04",
   "metadata": {},
   "source": [
    "We have everything we need now to tell the LLM to make some changes to our Anki flashcards.\n",
    "\n",
    "Let's pull a few note currently marked for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4449c78a-c525-43b3-96f4-cf665617105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_id = col.decks.current()[\"id\"]\n",
    "query = f\"did:{deck_id}\"\n",
    "note_ids = col.find_notes(query)\n",
    "\n",
    "NUM_NOTES_NEEDED = 10\n",
    "\n",
    "flagged_notes = []\n",
    "for note_id in note_ids:\n",
    "    if is_note_marked_for_review(col, note_id):\n",
    "        note = col.get_note(note_id)\n",
    "        flagged_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cad38b4-78b1-4790-8e4b-b2a77e763c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flagged notes: 284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of flagged notes: {len(flagged_notes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d6db93-fc16-43bb-b348-1e49e7df1e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)\n"
     ]
    }
   ],
   "source": [
    "from addon.application.services.formatter_service import AnkiNoteAdapter\n",
    "\n",
    "addon_note = AnkiNoteAdapter.to_addon_note(flagged_notes[0])\n",
    "print(addon_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af649819-1267-4ea3-910a-aac5265734aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\\n\\nAddonNote(front='Can bagging be performed in parallel?', back='Yes', guid='EbzX4(?/sg', tags=['ml'], notetype=<AddonNoteType.BASIC: 'basic'>, deck_name=None)\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"Look at this flashcard. How would you improve it? Keep in mind that flashcards should be atomic, concise, and accurate.\\n\\n{addon_note}\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eec669fd-203e-439c-8f94-75b9428c3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Content: <think>\n",
      "Okay, let's see. The user provided a flashcard about whether bagging can be performed in parallel. The front is the question, back is the answer. The user wants to know how to improve it, keeping in mind that flashcards should be atomic, concise, and accurate.\n",
      "\n",
      "First, I need to check if the current flashcard meets those criteria. The question is clear and direct, but maybe it's a bit vague. \"Can bagging be performed in parallel?\" is a yes/no question. The answer is \"Yes,\" which is concise. But maybe the answer could be more informative. The user might want to know why it's possible or the conditions under which it can be done.\n",
      "\n",
      "However, the original instruction says the flashcard should be atomic and concise. So adding more details might make it less concise. But maybe the answer could be a bit more precise. For example, explaining that bagging involves training models independently, which allows parallelization. But that might be too much for a flashcard.\n",
      "\n",
      "Another point: the tags are 'ml', which is correct. The note type is basic, which is fine. The deck name is None, which might not be an issue here. The GUID is probably not relevant for the user's question.\n",
      "\n",
      "So, the main improvement could be making the answer more precise. Instead of just \"Yes,\" maybe adding a brief reason. But the user said flashcards should be atomic and concise. So maybe the current answer is acceptable, but perhaps the question could be rephrased for clarity.\n",
      "\n",
      "Alternatively, the question could be split into two parts: one about whether it's possible and another about why. But that would make two flashcards. The original is a single card, so maybe that's not needed.\n",
      "\n",
      "Wait, the user's example has the front as \"Can bagging be performed in parallel?\" and back as \"Yes.\" So the improvement might be to make the answer more informative without being too long. Maybe \"Yes, because bagging involves training independent models in parallel.\" But that's adding a bit more context. However, the user's instruction is to improve the flashcard, so maybe that's acceptable if it's still concise.\n",
      "\n",
      "Alternatively, if the user wants the answer to be strictly yes/no, then the current answer is fine. But the original question might not be the best. Maybe the question could be more specific, like \"Is bagging suitable for parallel processing?\" or \"Can the individual models in bagging be trained in parallel?\"\n",
      "\n",
      "Another angle: the term \"bagging\" refers to Bootstrap Aggregating, which typically involves training multiple models on different subsets of data. Since each model is trained independently, they can be parallelized. So the answer is yes, but the flashcard might benefit from a slightly more accurate answer that mentions the independence of models.\n",
      "\n",
      "But again, the user wants it to be concise. So maybe the answer is okay as is, but the question could be rephrased for clarity. Alternatively, the answer could be \"Yes, as each model in bagging is trained independently and can be parallelized.\"\n",
      "\n",
      "But the user might prefer the answer to be as short as possible. So perhaps the original is acceptable, but the improvement is to ensure that the answer is accurate. Maybe the answer is correct, but the question could be more precise. For example, \"Can the individual models in a bagging ensemble be trained in parallel?\" instead of just \"Can bagging be performed in parallel?\" because \"bagging\" as a process might involve more steps than just training models.\n",
      "\n",
      "In summary, the main improvements would be:\n",
      "\n",
      "1. Make the question more specific to avoid ambiguity.\n",
      "2. Ensure the answer is accurate and concise, possibly adding a brief reason without being too lengthy.\n",
      "</think>\n",
      "\n",
      "The flashcard is concise and accurate, but it can be improved for clarity and educational value by adding context while maintaining brevity. Here's a refined version:\n",
      "\n",
      "**Front:**  \n",
      "\"Can bagging be parallelized?\"  \n",
      "\n",
      "**Back:**  \n",
      "\"Yes, because individual models are trained independently on different data subsets.\"  \n",
      "\n",
      "**Improvements:**  \n",
      "1. **Clarity:** \"Parallelized\" is more precise than \"performed in parallel.\"  \n",
      "2. **Context:** Briefly explains *why* bagging allows parallelization (independence of models and data subsets), enhancing understanding without overcomplicating.  \n",
      "3. **Conciseness:** Retains atomicity by avoiding unnecessary details.  \n",
      "\n",
      "This version maintains the flashcard's utility while improving its pedagogical value.\n",
      "Reasoning: None\n"
     ]
    }
   ],
   "source": [
    "content, reasoning_content = answer(\n",
    "    prompt=prompt,\n",
    "    mode=Mode.CHAT_COMPLETION, \n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c735cfe6-db80-45d6-89d3-72acf9cbaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01900f2a-bea3-45ec-b277-8c8166e607f1",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Check Qwen 3's instruction following capabilities with Completions and Chat Completions API\n",
    "- [ ] Simple prompt\n",
    "- [ ] Simple prompt + Chain of Thought\n",
    "- [ ] Two-step process (critique → refine)\n",
    "- [ ] Agent\n",
    "- [ ] Check if we have a class that we can use to read the collection from the hard disk and convert it to `AddonNote` instead of `Note`. In that way we can operate with domain objects instead of external dependencies. The same class should be used to convert the `AddonNote` back to `Note` (so maybe we should keep track of the `note_id`\n",
    "- [ ] Once we have that class, we should update the rest of the codebase accordingly (e.g., note formatter, note counter, etc.)\n",
    "- [ ] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c8e77-6198-4a26-b303-65546c7249cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
